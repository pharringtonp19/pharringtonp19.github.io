+++
title = "Causal Inference"
date = 2024-01-23
+++

At the individual level, causal inference is a "missing data" problem. Under the the potential outcome framework, we observe an individual in one state of the world. They are either treated or untreated. 

$$Y_i = D_i Y_i(1) + (1 - D_i) Y_i(0)$$

To overcome this challenge, we make use of observations across individuals. The typical approach in applied microeconomic papers is to fit a linear model to the data and interpret the coefficient associated with the treatment variable as the estimated average treatment effect.

---

One question that I've gotten on my job market paper is why go beyond the linear model. Why train feed-forward neural networks? Why fit fine-tuned language models?

Earlier on in graduate school, I would have given a rather poor response to this question. I would have said that we should prefer machine learning based models because they are more flexible and therefore don't suffer from model misspecification. While a factually correct statement, it's not a great answer because it doesn't highlight the tradeoffs that we should consider when using these models.

My current and hopefully more informed answer is that all of these models can be thought of as approximating a conditional expectation function defined with respect to the $\sigma$-algebra generated by the discrete toplogy.[^1] Given a finite sample, the model needs to generalize. That is, it might see in its training set that a tenanant who is experiencing financial difficulties because they loss their job and has three kids received the treatment, and must now predict on the validation set whether a tenant who has temporarily lost there job and has two kids is likely to receive treatment as well. The credibility of our causal estimates depend on the model's ability to generalize.[^2] 

All models work "roughly" by averaging over similar observations. 

$$\hat{y}(x) = \sum _i \alpha_i k(x, x_i)$$

With a linear model, we take the kernel to be the inner product in the feature space. We'll generalize well if (1) the inner product provides a good measure of similarity with respect to the take up rate of legal representation and (2) those assigned treatment don't have a latent feature which predicts the outcome. 
With a neural network, we impose less structure on (1) by allowing the model to learn the appropriate kernel (see [this paper](https://arxiv.org/abs/2012.00152)). With fine-tuned language models, we impose even less structure on (1) because we don't impose any topology, we're allowing the model to learn everything via pre-training & fine-tuning (see [this paper](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)). 

<div style="display:flex;">
  <div style="flex:1; max-width:48%;">
    <img src="/images/linear_signals.png" alt="image" style="max-width:100%; height:auto; object-fit:contain;">
  </div>
  <div style="flex:1; max-width:48%;">
    <img src="/images/llm_signals.png" alt="image" style="max-width:100%; height:auto; object-fit:contain;">
  </div>
</div>

As shown above, the inner product structure imposed by the linear model can be particularly poor. The iv-residuals for the those assigned the instrument should have positive residuals while those assigned the control should have negative residuals. The linear model fails to capture this structure while the language model does.

---

The initial missing data problem at the individual level is transformed at the dataset level into the problem of having to bet on something we either don't care about or have no control over. With linear models, it's the inner product strucuture, with feed-forward neural networks it's the hyperparameter values, and with language models it's the implicit embeddings.

Discussing the causal estimates returned from a model is about as comfortable as sitting on a stool with a partilly broken leg. You have no idea if the leg is going to snap out from under you. By fitting a variety of models (linear, feed-forward, and fine-tuned), your stool at least has three patially broken legs, which might make you feel a bit more comfortable. At the end of the day, it's up to you. 






<!-- At this "dataset level", the central challenge with causal inference is that we're betting on things we either don't care about or have no control over. With linear models, it's the inner product strucuture, with feed-forward neural networks, it's the hyperparameter values, and with language models, it's the implicit embeddings.  -->



[^1] I'm considering the underlying set to be the set of all possible sequences of words up to some length.

[^2] Generalization, not within variation






<!-- After you've taken your first undergraduate course in econometrics, the question your left asking yourself is how exactly does the potential outcome framework fit with linear models. 

$$Y_i = \alpha + \beta D_i + \gamma X_i + \varepsilon_i $$

A simple, yet imperfect way is to assume a constant treatment effect, which allows $\varepsilon_i$ to capture heterogeneity across individuals in the untreated outcome. 

Intuitively, this doesn't feel right. It feels uncomfortable. You would be sheepishly emarassed to present these results under this assumption to a broader audience. 

And yet, this is in some way what we all do, even when we are fully nonparametric and fitting flexible models. 

(How to tie in generalization here??)

In the finite sample, you can't meaningfully separate identification from estimation.  -->
